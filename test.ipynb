{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from preprocess import *\n",
    "from named_entity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # We read every book and its chapters\n",
    "    book1 = read_book(0)\n",
    "    book2 = read_book(1)\n",
    "    book3 = read_book(2)\n",
    "\n",
    "    # We remove the book name references\n",
    "    book2 = remove_book_name(book2, 'Ink 2 - Inkspell')\n",
    "    book3 = remove_book_name(book3, 'Ink 3 - Inkdeath')\n",
    "\n",
    "    # We divide each book in chapters (where book 3 has different chapter names)\n",
    "    ch1 = chop_chapters(book1)\n",
    "    ch2 = chop_chapters(book2)\n",
    "    ch3 = chop_chapters(book3, reg=r'CHAPTER [0-9]+')\n",
    "\n",
    "    # For the first two books we remove the quote/poem\n",
    "    ch1 = remove_quotes(ch1)\n",
    "    ch2 = remove_quotes(ch2)\n",
    "\n",
    "    # And for the third book we remove the chapter names\n",
    "    ch3 = remove_chapter_name(ch3)\n",
    "\n",
    "    return ch1, ch2, ch3\n",
    "\n",
    "def ne_extract(book, _num=''):\n",
    "    print(\"NE Extraction: \" + str(num))\n",
    "    # Create one big string from the whole book\n",
    "    text = \" \".join(list(itertools.chain.from_iterable(list(book.values())[1:-1])))\n",
    "    # Split the sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Tokenize and tag the sentences...\n",
    "    tokenizedSentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    taggedSentences = [nltk.pos_tag(sentence) for sentence in tokenizedSentences]\n",
    "    # ...and then create an nltk.tree.Tree from the sentences\n",
    "    chunkedSentences = nltk.ne_chunk_sents(taggedSentences, binary=True)\n",
    "\n",
    "    # Create a list of all the named entities\n",
    "    entityNames = buildDict(chunkedSentences)\n",
    "    entityNames = removeStopwords(entityNames)          # Remove the stop words\n",
    "    majorCharacters = getMajorCharacters(entityNames)   # And get occurences > 10\n",
    "\n",
    "    # Split the whole text in sentences using RegEx\n",
    "    sentenceList = splitIntoSentences(text)\n",
    "\n",
    "    # Compare the list of characters with each sentence\n",
    "    # returns a dict of all sentences for each character\n",
    "    characterSentences = compareLists(sentenceList, majorCharacters)\n",
    "\n",
    "    return characterSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1, book2, book3 = read_data()\n",
    "\n",
    "books = [book1, book2, book3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NE Extraction: Inkheart\n",
      "NE Extraction: By Funke, Cornelia\n",
      "NE Extraction: To Rolf, always--it was the best of things to be married to Dustfinger.\n"
     ]
    }
   ],
   "source": [
    "ne_books = [ne_extract(book, num) for num, book in enumerate(books)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Elinor',\n",
       " 'Orpheus',\n",
       " 'Magpie',\n",
       " 'Mortola',\n",
       " 'Meggie',\n",
       " 'Mortimer',\n",
       " 'Resa',\n",
       " 'Wayless Wood',\n",
       " 'Ombra',\n",
       " 'Silvertongue',\n",
       " 'Darius',\n",
       " 'Don',\n",
       " 'Black Prince',\n",
       " 'Mo',\n",
       " 'Black',\n",
       " 'Prince',\n",
       " 'Strong Man',\n",
       " 'Strong',\n",
       " 'Milksop',\n",
       " 'Adderhead',\n",
       " 'Snapper',\n",
       " 'Bluejay',\n",
       " 'Fenoglio',\n",
       " 'Wasn',\n",
       " 'Adder',\n",
       " 'Basta',\n",
       " 'Ombra Castle',\n",
       " 'Roxane',\n",
       " 'Battista',\n",
       " 'Gecko',\n",
       " 'Farid',\n",
       " 'Cheeseface',\n",
       " 'Chunk',\n",
       " 'Oss',\n",
       " 'Dustfinger',\n",
       " 'Minerva',\n",
       " 'Cosimo',\n",
       " 'White Women',\n",
       " 'Jink',\n",
       " 'Gwin',\n",
       " 'Rosenquartz',\n",
       " 'Brianna',\n",
       " 'Balbulus',\n",
       " 'Sootbird',\n",
       " 'Jasper',\n",
       " 'Ironstone',\n",
       " 'Piper',\n",
       " 'Violante',\n",
       " 'Ivo',\n",
       " 'Hadn',\n",
       " 'Jacopo',\n",
       " 'Tullio',\n",
       " 'White Book',\n",
       " 'Didn',\n",
       " 'Doria',\n",
       " 'Elfbane',\n",
       " 'Silver Prince',\n",
       " 'Capricorn',\n",
       " 'Despina',\n",
       " 'Cerberus',\n",
       " 'White Woman',\n",
       " 'Loredan',\n",
       " 'Signora Loredan',\n",
       " 'Lake']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test = list(ne_books[2].keys())\n",
    "removeStopwords(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd061d7c58a9914fd5194c3bd6d8bc24b3d05b6ef9cba948b7f78bd366acfe11009",
   "display_name": "Python 3.9.0  ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "61d7c58a9914fd5194c3bd6d8bc24b3d05b6ef9cba948b7f78bd366acfe11009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}