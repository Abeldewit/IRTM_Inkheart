{
 "cells": [
  {
   "source": [
    "# IRTM Project \n",
    "#### _Inkheart trilogy_ - **Abel de Wit**\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Loading\n",
    "Import all the necessary modules for this project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from main_functions import *\n",
    "from ast import literal_eval\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "source": [
    "Read in the data, if a dataFrame.csv has been created, this can be skipped and it can be read later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_1, book_2, book_3 = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools as it\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "idx1 = [('book_1', f'ch_{c}') for c in range(len(book_1))]\n",
    "idx2 = [('book_2', f'ch_{c}') for c in range(len(book_2))]\n",
    "idx3 = [('book_3', f'ch_{c}') for c in range(len(book_3))]\n",
    "idx1.extend(idx2)\n",
    "idx1.extend(idx3)\n",
    "idx = pd.MultiIndex.from_tuples(idx1, names=['book', 'chapter'])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index=idx)\n",
    "\n",
    "full_1 = [' '.join(sentences) for sentences in book_1.values()]\n",
    "full_2 = [' '.join(sentences) for sentences in book_2.values()]\n",
    "full_3 = [' '.join(sentences) for sentences in book_3.values()]\n",
    "\n",
    "df['text'] = [text.replace('Mortimer', 'Mo') for text in list(it.chain(full_1, full_2, full_3))]\n",
    "\n",
    "identity = str.maketrans(\"\", \"\", '()')\n",
    "# Split the text into sentences using sent_tokenize\n",
    "df['sentences'] = df['text'].progress_apply(lambda x: nltk.sent_tokenize(x.translate(identity)))\n",
    "# Split the sentences into tokens using word_tokenize\n",
    "df['tokens'] = df['sentences'].progress_apply(lambda x: [nltk.word_tokenize(sent) for sent in x])\n",
    "# Give each token a pos tag\n",
    "df['pos'] = df['tokens'].progress_apply(lambda x: [nltk.pos_tag(sent) for sent in x])\n",
    "\n",
    "# Create both an 'unspecific' and 'specific' tree for the named entity recoginition\n",
    "df['binary_tree'] = df['pos'].progress_apply(lambda x: [str(tree) for tree in nltk.ne_chunk_sents(x, binary=True)])\n",
    "df['tree'] = df['pos'].progress_apply(lambda x: [str(tree) for tree in nltk.ne_chunk_sents(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_person(tree, _entityNames = None):\n",
    "    \"\"\"\n",
    "    Loopts trough the nltk tree so find PERSON's\n",
    "    returns a list of entity names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if _entityNames is None:\n",
    "            _entityNames = []\n",
    "\n",
    "        try:\n",
    "            label = tree.label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if label == 'PERSON':\n",
    "                # print(label, tree[0].split('/')[0])\n",
    "                _entityNames.append(tree[0].split('/')[0])\n",
    "            else:\n",
    "                for child in tree:\n",
    "                    extract_person(child, _entityNames)\n",
    "    except ValueError:\n",
    "        print(tree)\n",
    "    return _entityNames\n",
    "\n",
    "def extract_chap_person(sent_list):\n",
    "    e_names = []\n",
    "    for sent in sent_list:\n",
    "        tree = nltk.Tree.fromstring(sent)\n",
    "        e_names.append(extract_person(tree))\n",
    "\n",
    "    return removeStopwords(e_names)\n",
    "\n",
    "def removeStopwords(entityNames, customStopWords=None):\n",
    "    \"\"\"\n",
    "    The nltk pos tagger has some difficulties with standard English words\n",
    "    that are at the beginning of a sentence \n",
    "    (e.g. \"Try that new shampoo\" -> ('Try', NE))\n",
    "    Hence, we remove stopwords from the possible character list, \n",
    "    the stopwords are extended with custom words that were added manually\n",
    "    \"\"\"\n",
    "    filterNames = []\n",
    "    if customStopWords is None:\n",
    "        with open(\"./customStopWords.txt\", \"r\") as f:\n",
    "            customStopWords = f.read().split(', ')\n",
    "\n",
    "    for name in entityNames:\n",
    "        if (name not in stopwords.words('english')) and (name not in customStopWords):\n",
    "            filterNames.append(name)\n",
    "\n",
    "    return filterNames\n",
    "\n",
    "# Create a new column for the named entities\n",
    "df['named_entities'] = df['tree'].progress_apply(lambda x: extract_chap_person(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dict(ne_list):\n",
    "    \"\"\"\n",
    "    Counts how often a named entity occurs in a chapter\n",
    "    returns a dictionary with the names as keys and the occurrences as values\n",
    "    \"\"\"\n",
    "    counter = {}\n",
    "    for ne in itertools.chain(ne_list):\n",
    "        for name in ne:\n",
    "            if name in counter.keys():\n",
    "                counter[name] += 1\n",
    "            else:\n",
    "                counter[name] = 1\n",
    "    # Remove occurrences less than 2, usually these are misfires\n",
    "    return {key: value for key, value in counter.items() if value > 2}\n",
    "\n",
    "df['ne_count'] = df['named_entities'].progress_apply(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df.to_csv('./dataFrame.csv')"
   ]
  },
  {
   "source": [
    "If the dataFrame.csv has been provided, it can be read here without running the cells above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the saved dataframe\n",
    "cols = list(df.columns)\n",
    "conv = dict.fromkeys(cols[1:], literal_eval)\n",
    "df = pd.read_csv('./dataFrame.csv', index_col=[0,1], converters=conv)\n",
    "df.head(2)"
   ]
  },
  {
   "source": [
    "## Character extraction & visualisation\n",
    " Now it's time to process the data and get some information retrieval going"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a set of unique names found by the NER\n",
    "all_names = set(list(itertools.chain.from_iterable(list(df['ne_count'].apply(lambda x: list(x.keys()))))))\n",
    "\n",
    "tmp_dict = {key: val for key, val in time_name_dict.items()}\n",
    "tmp = {val: sum(idx for idx in key) for val, key in tmp_dict.items()}\n",
    "tmp2 = sorted(tmp_dict.items(), key = lambda ele: tmp[ele[0]], reverse=True)\n",
    "tmp_dict = {key: val for key, val in tmp2}\n",
    "\n",
    "# We take only the top 20 occuring names\n",
    "names = list(tmp_dict.keys())[:20]\n",
    "\n",
    "def plot_occurence(df_count):\n",
    "    # Create an array over the 'time' in the books of occurrences of each name\n",
    "    time_name_dict = {}\n",
    "    for name in names:\n",
    "        _tmp = list(df_count.apply(lambda x: [value for key, value in x.items() if key == name]))\n",
    "        _counts = np.array([0 if len(x)==0 else x[0] for x in _tmp])\n",
    "        time_name_dict[name] = _counts\n",
    "\n",
    "    # Create a list of all book+chapter names (for the plot)\n",
    "    X_1 = [f'B1C{chap}' for chap in range(len(book_1))]\n",
    "    X_2 = [f'B2C{chap}' for chap in range(len(book_2))]\n",
    "    X_3 = [f'B3C{chap}' for chap in range(len(book_3))]\n",
    "\n",
    "    # Sort the sum of occurences so the most 'prominent' characters are on top\n",
    "    books_Y = []\n",
    "    lenlist = [\n",
    "        (0, len(book_1)), \n",
    "        (len(book_1), len(book_1)+len(book_2)), \n",
    "        (len(book_1)+len(book_2), len(book_1)+len(book_2)+len(book_3))\n",
    "    ]\n",
    "\n",
    "    # Split the list in the three books for a less 'cluttered' graph\n",
    "    for b, e in lenlist:\n",
    "        tmp_dict = {key: val[b:e] for key, val in time_name_dict.items()}\n",
    "        tmp = {val: sum(idx for idx in key) for val, key in tmp_dict.items()}\n",
    "        tmp2 = sorted(tmp_dict.items(), key = lambda ele: tmp[ele[0]], reverse=True)\n",
    "        tmp_dict = {key: val for key, val in tmp2}\n",
    "        books_Y.append(tmp_dict)\n",
    "\n",
    "    # Choose how many characters to show, default is 10\n",
    "    CHAR_NUM = 10\n",
    "\n",
    "    xylist = []\n",
    "    for X, Y, name in [(X_1, books_Y[0], 'Book 1'), (X_2, books_Y[1], 'Book 2'), (X_3, books_Y[2], 'Book 3')]:\n",
    "        fig = plt.figure(figsize=(30, 5))\n",
    "        plt.stackplot(X, list(Y.values())[:CHAR_NUM], baseline='sym')\n",
    "        plt.ylabel('Occurence')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend(list(Y.keys())[:CHAR_NUM])\n",
    "        plt.title(f'Character occurence in {name}')\n",
    "        plt.savefig(f'./images/{name}.png')\n",
    "        plt.show()\n",
    "        xylist.append((X, Y))\n",
    "\n",
    "\n",
    "plot_occurence(df['ne_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "source": [
    "### Resolve co-references to get more accurate information about character occurences and interactions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define some weird character names that the model might have difficulty with\n",
    "name_definitions = {\n",
    "    'Capricorn': ['man'],\n",
    "    'Dustfinger': ['man'],\n",
    "    'Elinor': ['woman'],\n",
    "    'Meggie': ['girl'],\n",
    "    'Mo': ['man', 'father', 'Silvertongue']\n",
    "}\n",
    "\n",
    "# Create the model with the specified vocabulary and character names\n",
    "coref = neuralcoref.NeuralCoref(\n",
    "    nlp.vocab, \n",
    "    conv_dict=name_definitions)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a SpaCy doc for each chapter in the dataframe\n",
    "df['doc'] = df['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uses the previous created list of names to only resolve those co-references in the text\n",
    "def resolve_coref(doc, names):\n",
    "    tokens = [tok.text for tok in doc]\n",
    "\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        name = cluster.main.text\n",
    "        mentions = cluster.mentions\n",
    "\n",
    "        if name in names:\n",
    "            for ment in mentions:\n",
    "                s = ment.start\n",
    "                e = ment.end\n",
    "\n",
    "                tokens[s:e] = [name if i == 0 else '' for i in range(e-s)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Create a new column with text in which each co-reference is resolved\n",
    "df['coref_resolve'] = df['doc'].apply(lambda x: resolve_coref(x, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts ofccurences of a list of provided names in the provided text\n",
    "def count_occurence(text, names):\n",
    "    name_dict = {}\n",
    "    for name in names:\n",
    "        c = text.count(name)\n",
    "        name_dict[name] = c\n",
    "    return name_dict\n",
    "\n",
    "# A different count now that all co-references have been resolved\n",
    "df['res_ne_count'] = df['coref_resolve'].apply(lambda x: count_occurence(x, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xylist2 = plot_occurence(df['res_ne_count'])"
   ]
  },
  {
   "source": [
    "Let's see if the co-references that have been resolved make sense, we will take random samples from the co-references and show where and how it would be subsituted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df['doc'])\n",
    "\n",
    "# A function to show whether a co-reference has been resolved correctly or not\n",
    "all_refs = {}\n",
    "for doc in docs:\n",
    "    clusters = doc._.coref_clusters\n",
    "    for cluster in clusters:\n",
    "        if cluster.main.text not in names:\n",
    "            continue\n",
    "        if cluster.main.text in all_refs.keys():\n",
    "            all_refs[cluster.main.text].extend(cluster.mentions)\n",
    "        else:\n",
    "            all_refs[cluster.main.text] = cluster.mentions\n",
    "\n",
    "for name in all_refs.keys():\n",
    "    mentions = all_refs[name]\n",
    "    sample = random.randint(0, len(mentions)-1)\n",
    "\n",
    "    test = mentions[sample]\n",
    "    doc = test.doc\n",
    "    if test.text == name:\n",
    "        continue\n",
    "    window = 20\n",
    "    if window > test.start:\n",
    "        window = test.start\n",
    "    print(doc[test.start-window:test.start], f'[ {doc[test.start]}/{name} ]', doc[test.end:test.end+window], '\\n')\n",
    "    "
   ]
  },
  {
   "source": [
    "### Error rate testing\n",
    "Now we run the same method as above, but ask the user each time to say whether the coreference was correct. This way we can accumulate an error rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assesment = {'good': 0, 'bad': 0, 'neutral': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test will run untill the user stops it with input 't'\n",
    "i = ''\n",
    "while i != 't':\n",
    "    name_sample = random.randint(0, len(all_refs.keys())-1)\n",
    "    name = list(all_refs.keys())[name_sample]\n",
    "\n",
    "    mentions = all_refs[name]\n",
    "    sample = random.randint(0, len(mentions)-1)\n",
    "\n",
    "    test = mentions[sample]\n",
    "    doc = test.doc\n",
    "    if test.text == name:\n",
    "        continue\n",
    "    window = 40\n",
    "    if window > test.start:\n",
    "        window = test.start\n",
    "    # Prints a random sample from both the names and sentences\n",
    "    print(doc[test.start-window:test.start], f'\\n[ {doc[test.start]}/{name} ]\\n', doc[test.end:test.end+window], '\\n')\n",
    "\n",
    "    i = input(\"(a/good, s/neutral, d/bad): \")  # Ask user for input\n",
    "\n",
    "    if i.lower() == 'a':\n",
    "        assesment['good'] += 1\n",
    "    elif i.lower() == 's':\n",
    "        assesment['neutral'] += 1\n",
    "    elif i.lower() == 'd':\n",
    "        assesment['bad'] += 1\n",
    "    elif i.lower() == 't':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Wrong command, nothing is graded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(assesment.keys(), assesment.values())\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Relation mapping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coref_sentences = list(df['coref_resolve'].apply(nltk.sent_tokenize))\n",
    "\n",
    "relation_dict = {}\n",
    "for name1, name2 in itertools.product(names, names):\n",
    "    if name1 == name2:\n",
    "        continue\n",
    "    if (name2, name1) in relation_dict.keys():\n",
    "        continue\n",
    "\n",
    "    combination_list = []\n",
    "    for i, chap in enumerate(coref_sentences):\n",
    "        chap_list = []\n",
    "        for j, sent in enumerate(chap):\n",
    "            if name1 in sent and name2 in sent:\n",
    "                chap_list.append(1)\n",
    "            else:\n",
    "                chap_list.append(0)\n",
    "        combination_list.append(chap_list)\n",
    "    relation_dict[(name1, name2)] = combination_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for combination in relation_dict.keys():\n",
    "    momeg = np.array([sum(chap) for chap in list(relation_dict[combination])])\n",
    "    fig = plt.figure(figsize=(2, 1))\n",
    "    plt.title(combination)\n",
    "    plt.plot(momeg)\n",
    "    plt.show()\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an 'adjacency' matrix with weights for occurences\n",
    "adjacency = np.zeros(shape=(len(names), len(names)))\n",
    "\n",
    "for i, name1 in enumerate(names):\n",
    "    for j, name2 in enumerate(names):\n",
    "        if (name1, name2) in relation_dict.keys():\n",
    "            adjacency[i, j] = sum(itertools.chain.from_iterable(relation_dict[(name1, name2)]))\n",
    "            adjacency[j, i] = sum(itertools.chain.from_iterable(relation_dict[(name1, name2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(\n",
    "    adjacency,\n",
    "    xticklabels=names,\n",
    "    yticklabels=names)\n",
    "plt.savefig('./images/bigheat.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = 10\n",
    "sns.heatmap(\n",
    "    adjacency[:top_num, :top_num],\n",
    "    xticklabels=names[:top_num],\n",
    "    yticklabels=names[:top_num])\n",
    "plt.savefig('./images/smallheat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "for name in names:\n",
    "    graph.add_node(name)\n",
    "\n",
    "for i, name1 in enumerate(names):\n",
    "    for j, name2 in enumerate(names):\n",
    "        graph.add_weighted_edges_from([(name1, name2, adjacency[i, j])])\n",
    "nx.draw_networkx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_name_dict = {}\n",
    "for name in names:\n",
    "        _tmp = list(df['ne_count'].apply(lambda x: [value for key, value in x.items() if key == name]))\n",
    "        _counts = np.array([0 if len(x)==0 else x[0] for x in _tmp])\n",
    "        time_name_dict[name] = _counts\n",
    "\n",
    "name_size = {key: sum(val) for key, val in time_name_dict.items()}\n",
    "\n",
    "sizes = {key: {'size': val} for key, val in name_size.items()}\n",
    "\n",
    "for name in name_size.keys():\n",
    "    nx.set_node_attributes(graph, sizes)\n",
    "\n",
    "nx.draw_networkx(graph)\n",
    "nx.write_graphml(graph, 'relation_graph.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the same but over each chapter to create a dynamic graph\n",
    "\n",
    "its = [(key, key, value) for key, value in name_size.items()]\n",
    "node_df = pd.DataFrame.from_records(its, columns=['Id', 'Label', 'Size'])\n",
    "print(node_df.head())\n",
    "node_df.to_csv('node_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for combination in relation_dict.keys():\n",
    "    adjacency = np.zeros(shape=(len(relation_dict.keys()), len(relation_dict.keys())))\n",
    "    chap_list = np.array([sum(chap) for chap in list(relation_dict[combination])])\n",
    "    edges.extend([(combination[0], combination[1], chap, f'<[{i},{i}]>') for i, chap in enumerate(chap_list)])\n",
    "\n",
    "edge_df = pd.DataFrame.from_records(edges, columns=['Source', 'Target', 'Weight', r'TimeSet'])\n",
    "edge_df.to_csv('edge_test.csv', index=False)"
   ]
  },
  {
   "source": [
    "These `csv` files can be read by the application 'Gephi' which is able to both display and arange the nodes and edges, as well as showing the edge weights over time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Adjective extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chapter = df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = first_chapter['doc']\n",
    "\n",
    "adj_dict = {}\n",
    "for doc in list(df['doc']):\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        \n",
    "        # print(token.text, token.pos_, token.dep_)\n",
    "        for child in children:\n",
    "            if child.pos_ == 'ADJ':\n",
    "                if token.text in names:\n",
    "                    # print('-'*10)\n",
    "                    # print(child.text, token.text, token.pos_)\n",
    "                    if token.text in adj_dict.keys():\n",
    "                        adj_dict[token.text].append(child.text)\n",
    "                    else:\n",
    "                        adj_dict[token.text] = [child.text]\n",
    "\n",
    "\n",
    "adj_dict"
   ]
  },
  {
   "source": [
    "## Location extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in itertools.chain.from_iterable([[(i, j) for j, sent in enumerate(sentences) if 'castle' in sent] for i, sentences in enumerate(list(df['sentences']))])][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locations = [\n",
    "    'the castle of night',\n",
    "    ''\n",
    "]\n",
    "\n",
    "\n",
    "for chap in list(df['doc']):\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            if child.pos_ in [ 'PROPN']:\n",
    "                print(token, child, child.pos_)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def traverse_tree(tree):\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            gpe_name = ' '.join(subtree).split('/')\n",
    "            if len(gpe_name) > 2:\n",
    "                print(gpe_name, subtree)\n",
    "            if gpe_name[0] not in names:\n",
    "                print(gpe_name, subtree)\n",
    "            traverse_tree(subtree)\n",
    "        \n",
    "\n",
    "\n",
    "for chap in list(df['tree']):\n",
    "    for sent in chap:\n",
    "        tree = nltk.Tree.fromstring(sent)\n",
    "        traverse_tree(tree)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 2823305\n",
      "castle of ombra 4\n",
      "inn by the border 0\n",
      "inn 188\n",
      "wayless wood 64\n",
      "capricorn's fortress 0\n",
      "fortress 41\n",
      "argenta 19\n",
      "lombrica 9\n",
      "castle of night 163\n",
      "molley folk 0\n",
      "the inn of the 0\n",
      "the spelt-mill 7\n",
      "the infirmary 11\n",
      "mount adder 17\n"
     ]
    }
   ],
   "source": [
    "test = 'Mount Adder'\n",
    "\n",
    "i = ''\n",
    "locations = []\n",
    "while i != 't':\n",
    "    s = sum([' '.join(text).lower().count(i) for text in [full_1, full_2, full_3]])\n",
    "    print(i, s)\n",
    "    if s > 2:\n",
    "        locations.append(i)\n",
    "    i = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['',\n",
       " 'castle of ombra',\n",
       " 'inn',\n",
       " 'wayless wood',\n",
       " 'fortress',\n",
       " 'argenta',\n",
       " 'lombrica',\n",
       " 'castle of night',\n",
       " 'the spelt-mill',\n",
       " 'the infirmary',\n",
       " 'mount adder']"
      ]
     },
     "metadata": {},
     "execution_count": 389
    }
   ],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "locations = []\n",
    " 'castle of ombra',\n",
    " 'inn',\n",
    " 'wayless wood',\n",
    " 'fortress',\n",
    " 'argenta',\n",
    " 'lombrica',\n",
    " 'castle of night',\n",
    " 'the spelt-mill',\n",
    " 'the infirmary',\n",
    " 'mount adder'\n",
    "]\n",
    "\n",
    "for tokenlist in itertools.chain.from_iterable(list(df['tokens'])):\n",
    "    loc_bool = [[True if loc in tokenlist else False for loc in location] for location in locations]\n",
    "    for location in loc_bool:\n",
    "        if True in location:\n",
    "            print(tokenlist)\n"
   ]
  },
  {
   "source": [
    "### Dependency mapping\n",
    "\n",
    "Next step is to use the characters names and locations to show changes of scenery for each character"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def make_graph(doc):\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token.lower_),\n",
    "                        '{0}'.format(child.lower_)))\n",
    "\n",
    "    graph = nx.Graph(edges)\n",
    "    return graph\n",
    "\n",
    "df['graphs'] = df['doc'].apply(make_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 9))\n",
    "nx.draw_networkx(df['graphs'].iloc[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the length and path\n",
    "\n",
    "for i, graph in enumerate(list(df['graphs'])):\n",
    "    entity1 = 'Meggie'.lower()\n",
    "    entity2 = 'castle'.lower()\n",
    "    try:\n",
    "        print(nx.shortest_path_length(graph, source=entity1, target=entity2))\n",
    "        print(i, nx.shortest_path(graph, source=entity1, target=entity2))\n",
    "\n",
    "        path = nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "        for sent in df['sentences'].iloc[i]:\n",
    "            path_sent = True\n",
    "            for word in path:\n",
    "                if word not in sent.lower():\n",
    "                    path_sent = False\n",
    "            if path_sent:\n",
    "                print(sent)\n",
    "    except nx.NodeNotFound:\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd061d7c58a9914fd5194c3bd6d8bc24b3d05b6ef9cba948b7f78bd366acfe11009",
   "display_name": "Python 3.9.0  ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "61d7c58a9914fd5194c3bd6d8bc24b3d05b6ef9cba948b7f78bd366acfe11009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}